# Session Log

> Version: 0.2
> Input: Each AI-assisted work session
> Output: Reviewable record of what happened, decisions made, and outcomes

> Log every AI session. One entry per Heartbeat. This is how the team learns what works and what doesn't.

---

## Session: [Task Name]

**Date:** YYYY-MM-DD
**Engineer:** [name]
**Feature:** [parent feature name]
**Heartbeat:** [e.g., T3 — Event Publisher]
**AI Tool:** [e.g., Cursor + Claude Sonnet 4, ChatGPT-4o]
**Mode:** A / B (Heartbeat # of N)

### Inputs

What did you feed to the AI session?

- [ ] project-context.md
- [ ] decisions.md
- [ ] Task description from task-breakdown.md
- [ ] Existing code files: [list which files]
- [ ] DB schema
- [ ] Other: [specify]

### Session Timeline

| Time | What happened |
|---|---|
| 00:00 | Started session, pasted context + task |
| 00:05 | [e.g., AI proposed initial structure, looked reasonable] |
| 00:15 | [e.g., Had to correct naming convention — AI used snake_case instead of camelCase] |
| 00:30 | [e.g., AI hallucinated a method that doesn't exist in our ORM, rewrote manually] |
| 00:45 | [e.g., First working version, moved to review] |

### Context Rot Signals

Did any of these happen? Mark if yes.

- [ ] AI proposed APIs that "almost" exist but not quite
- [ ] Had to correct naming conventions mid-session
- [ ] Tests passed but behavior felt "off"
- [ ] Explained the same thing twice in one session
- [ ] Wasn't sure if code still matched the brief
- [ ] **Action taken:** [e.g., refreshed context / continued / aborted]

### Kill Switch

- **Was the kill switch triggered?** Yes / No
- **If yes, why?** [what went wrong]
- **What did you do instead?** [smaller task / wrote manually / different approach]

### Review Findings

**AI Review pass:**

[What did AI flag?]

**Human Review pass:**

[What did you catch that AI missed?]

**Hallucinations caught:**

| What AI generated | What actually exists | Impact |
|---|---|---|
| [e.g., `db.webhook.upsert()`] | [e.g., method doesn't exist, need `db.webhook.create()` + `db.webhook.update()`] | [e.g., would have thrown runtime error] |

### Output

| Metric | Value |
|---|---|
| Lines generated by AI | |
| Lines kept after review | |
| Lines written manually | |
| Static analysis issues | |
| Hallucinations caught | |
| Total session time | |
| Estimated time without AI | |

### Learnings

**What worked well in this session?**

-

**What didn't work?**

-

**What did you update in project-context.md?**

-

**One-line takeaway:** [e.g., "AI is great at BullMQ boilerplate but can't handle our custom retry logic"]

---

*(Copy the section above for each new session. Reverse chronological order.)*
